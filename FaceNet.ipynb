{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FaceNet single-predictor models\n",
    "Set of models investigating face perception:\n",
    "- single-predictor model with binary regressor coding for presence of any face (**any_faces**)\n",
    "- **any_faces** + regressor coding for first appearance face for the first time (**first_time_face**)\n",
    "- **any_faces** + log of time since last appearance of detected face (mean across faces if more than one are present in the frame, **log_mean_time_since**) \n",
    "- **any_faces** + log of time since last appearance of detected face (max across multiple faces, **log_max_time_since**) \n",
    "- **any_faces** + log of cumulative time the detected face has been on screen (mean across faces, **log_mean_face_time_cum**)\n",
    "- **any_faces** + log of cumulative time the detected face has been on screen (max across faces, **log_max_face_time_cum**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.create import create_set_models\n",
    "from tools.utils import dump_collection, load_collection\n",
    "from tools.viz import (plot_regressor, plot_regressor,\n",
    "                       plot_metrics, plot_contrast_by_dataset, plot_contrast_by_analysis,\n",
    "                       plot_analysis_grid)\n",
    "from pyns import Neuroscout\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = Neuroscout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define predictors and confounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [['any_faces']]\n",
    "other = ['first_time_face', 'log_mean_time_since', 'log_max_time_since', 'log_mean_face_time_cum', 'log_max_face_time_cum']\n",
    "for p in other:\n",
    "    predictors.append(['any_faces', p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds = ['a_comp_cor_00', 'a_comp_cor_01', 'a_comp_cor_02', 'a_comp_cor_03', 'a_comp_cor_04', 'a_comp_cor_05', 'trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mdict = {}\n",
    "# for pset in predictors:\n",
    "#    mdict['+'.join(pset)] = create_set_models(pset, confounds, name='+'.join(pset), datasets=['NaturalisticNeuroimagingDatabase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path('models') / 'facenet.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdict = load_collection(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump_collection(mdict, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FaceNet -- face switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = ['first_time_face', 'log_max_time_since', 'any_faces']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = [\n",
    "    {\n",
    "        'Input': ['log_max_time_since', 'first_time_face'],\n",
    "        'Name': 'ToDense',\n",
    "        'SamplingRate': 2\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        'Input': ['log_max_time_since'],\n",
    "        'Name': 'Threshold',\n",
    "        'Threshold': 0.5,\n",
    "        'Binarize': True,\n",
    "        'Output': ['time_since_temp']\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        'Input': ['time_since_temp', 'first_time_face'],\n",
    "        'Output': 'face_switch',\n",
    "        'Name': 'Or',\n",
    "    },\n",
    "    {\n",
    "        'Input': ['any_faces', 'face_switch'],\n",
    "        'Name': 'Convolve'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mdict['any_faces+face_switch'] = create_set_models(predictors=preds, confounds=confounds, name='any_faces+face_switch', transformations=transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for analysis in mdict['any_faces+face_switch']:\n",
    "    an = analysis['analysis']\n",
    "    an.model['Steps'][0]['Model']['X'] = ['face_switch', 'any_faces',\n",
    "         'a_comp_cor_00',\n",
    "         'a_comp_cor_01',\n",
    "         'a_comp_cor_02',\n",
    "         'a_comp_cor_03',\n",
    "         'a_comp_cor_04',\n",
    "         'a_comp_cor_05',\n",
    "         'trans_x',\n",
    "         'trans_y',\n",
    "         'trans_z',\n",
    "         'rot_x',\n",
    "         'rot_y',\n",
    "         'rot_z'\n",
    "    ]\n",
    "    an.model['Steps'][0]['DummyContrasts']['Conditions'] = ['face_switch', 'any_faces']\n",
    "    an.model['Steps'][0]['Transformations'] = transformations\n",
    "    an.push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for analysis in mdict['any_faces+face_switch']:\n",
    "    if analysis['dataset'] == 'NaturalisticNeuroimagingDatabase':\n",
    "        an = analysis['analysis']\n",
    "        an.model['Steps'][0]['Model']['X'] = ['face_switch', 'any_faces']\n",
    "        an.push()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_name = 'any_faces+face_switch+log_mean_time_since+first_time_face+log_mean_face_time_cum'\n",
    "transformations = [\n",
    "    {\n",
    "        'Input': ['log_mean_time_since', 'first_time_face'],\n",
    "        'Name': 'ToDense',\n",
    "        'SamplingRate': 2\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        'Input': ['log_mean_time_since'],\n",
    "        'Name': 'Threshold',\n",
    "        'Threshold': 0.5,\n",
    "        'Binarize': True,\n",
    "        'Output': ['time_since_temp']\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        'Input': ['time_since_temp', 'first_time_face'],\n",
    "        'Output': 'face_switch',\n",
    "        'Name': 'Or',\n",
    "    },\n",
    "    {\n",
    "        'Input': ['any_faces', 'face_switch', 'first_time_face', 'log_mean_time_since', 'log_mean_face_time_cum'],\n",
    "        'Name': 'Convolve'\n",
    "    }\n",
    "]\n",
    "\n",
    "all_preds = ['first_time_face', 'log_mean_time_since', 'any_faces', 'log_mean_face_time_cum']\n",
    "# mdict[all_name] = create_set_models(\n",
    "#     predictors=all_preds, confounds=confounds, name=name, transformations=transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "for analysis in mdict[all_name]:\n",
    "    an = analysis['analysis']\n",
    "    if dataset != 'NaturalisticNeuroimagingDatabase':\n",
    "        all_inputs = all_preds + ['face_switch'] + confounds\n",
    "    else:\n",
    "        all_inputs = all_preds + ['face_switch']\n",
    "    an.model['Steps'][0]['Model']['X'] = all_inputs\n",
    "    an.model['Steps'][0]['DummyContrasts']['Conditions'] = all_inputs\n",
    "    an.model['Steps'][0]['Transformations'] = transformations\n",
    "    an.push()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### face_switch + time_cum + speech + shot_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_speech_shot_name = '+'.join(hrf_vars)\n",
    "hrf_vars = ['speech', 'shot_change','face_switch', 'log_mean_face_time_cum']\n",
    "transformations = [\n",
    "    {\n",
    "        'Input': ['log_mean_time_since', 'first_time_face'],\n",
    "        'Name': 'ToDense',\n",
    "        'SamplingRate': 2\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        'Input': ['log_mean_time_since'],\n",
    "        'Name': 'Threshold',\n",
    "        'Threshold': 0.5,\n",
    "        'Binarize': True,        \n",
    "    },\n",
    "    {\n",
    "        'Input': ['log_mean_time_since', 'first_time_face'],\n",
    "        'Output': 'face_switch',\n",
    "        'Name': 'Or',\n",
    "    },\n",
    "    {\n",
    "        'Input': hrf_vars,\n",
    "        'Name': 'Convolve'\n",
    "    }\n",
    "]\n",
    "\n",
    "input_preds = ['first_time_face', 'log_mean_time_since', 'log_mean_face_time_cum', 'shot_change', 'speech']\n",
    "# mdict[all_speech_shot_name] = create_set_models(\n",
    "#     predictors=input_preds, confounds=confounds, name=all_speech_shot_name, transformations=transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "for analysis in mdict[all_speech_shot_name]:\n",
    "    an = analysis['analysis']\n",
    "    an.model['Steps'][0]['Model']['X'] = hrf_vars + confounds\n",
    "    an.model['Steps'][0]['DummyContrasts']['Conditions'] = hrf_vars\n",
    "    an.model['Steps'][0]['Transformations'] = transformations\n",
    "    an.push()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anyfaces + above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf_vars = ['any_faces', 'speech', 'shot_change','face_switch', 'log_mean_face_time_cum']\n",
    "name = '+'.join(hrf_vars)\n",
    "transformations = [\n",
    "    {\n",
    "        'Input': ['log_mean_time_since', 'first_time_face'],\n",
    "        'Name': 'ToDense',\n",
    "        'SamplingRate': 2\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        'Input': ['log_mean_time_since'],\n",
    "        'Name': 'Threshold',\n",
    "        'Threshold': 0.5,\n",
    "        'Binarize': True,        \n",
    "    },\n",
    "    {\n",
    "        'Input': ['log_mean_time_since', 'first_time_face'],\n",
    "        'Output': 'face_switch',\n",
    "        'Name': 'Or',\n",
    "    },\n",
    "    {\n",
    "        'Input': hrf_vars,\n",
    "        'Name': 'Convolve'\n",
    "    }\n",
    "]\n",
    "\n",
    "input_preds = ['any_faces', 'first_time_face', 'log_mean_time_since', 'log_mean_face_time_cum', 'shot_change', 'speech']\n",
    "# mdict[name] = create_set_models(\n",
    "#     predictors=input_preds, confounds=confounds, name=name, transformations=transformations)\n",
    "\n",
    "for analysis in mdict[name]:\n",
    "    an = analysis['analysis']\n",
    "    an.model['Steps'][0]['Model']['X'] = hrf_vars + confounds\n",
    "    an.model['Steps'][0]['DummyContrasts']['Conditions'] = hrf_vars\n",
    "    an.model['Steps'][0]['Transformations'] = transformations\n",
    "    an.push()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "Model w/ too many correlated predictors was less consistent across datasets\n",
    "\n",
    "\n",
    "Models to run:\n",
    "\n",
    "\n",
    "- \"back up\" plan: pairwise models including speech, and face_switch + shot change\n",
    "\n",
    "- face_switch + time_since + first_timeface (lower priority)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## any_faces + speech + shot_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf_vars = ['any_faces', 'speech', 'shot_change']\n",
    "name = '+'.join(hrf_vars)\n",
    "\n",
    "# mdict[name] = create_set_models(\n",
    "#     predictors=hrf_vars, confounds=confounds, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## any_faces + speech + shot_change + face_switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf_vars = ['any_faces', 'speech', 'shot_change','face_switch']\n",
    "name = '+'.join(hrf_vars)\n",
    "transformations = [\n",
    "    {\n",
    "        'Input': ['log_mean_time_since', 'first_time_face'],\n",
    "        'Name': 'ToDense',\n",
    "        'SamplingRate': 2\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        'Input': ['log_mean_time_since'],\n",
    "        'Name': 'Threshold',\n",
    "        'Threshold': 0.5,\n",
    "        'Binarize': True,        \n",
    "    },\n",
    "    {\n",
    "        'Input': ['log_mean_time_since', 'first_time_face'],\n",
    "        'Output': 'face_switch',\n",
    "        'Name': 'Or',\n",
    "    },\n",
    "    {\n",
    "        'Input': hrf_vars,\n",
    "        'Name': 'Convolve'\n",
    "    }\n",
    "]\n",
    "\n",
    "input_preds = ['any_faces', 'first_time_face', 'log_mean_time_since', 'shot_change', 'speech']\n",
    "# mdict[name] = create_set_models(\n",
    "#     predictors=input_preds, confounds=confounds, name=name, transformations=transformations)\n",
    "\n",
    "for analysis in mdict[name]:\n",
    "    an = analysis['analysis']\n",
    "    an.model['Steps'][0]['Model']['X'] = hrf_vars + confounds\n",
    "    an.model['Steps'][0]['DummyContrasts']['Conditions'] = hrf_vars\n",
    "    an.model['Steps'][0]['Transformations'] = transformations\n",
    "    an.push()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### any_faces + speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf_vars = ['any_faces', 'speech']\n",
    "name = '+'.join(hrf_vars)\n",
    "\n",
    "# mdict[name] = create_set_models(predictors=hrf_vars, confounds=confounds, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for di in mdict[name]:\n",
    "    sub_id = api.runs.get(di['analysis'].runs[0])['subject']\n",
    "    run_ids = [r['id'] for r in api.runs.get(subject=sub_id)]\n",
    "    run_ids = [r for r in run_ids if r in di['analysis'].runs]\n",
    "    di['analysis'].generate_report(run_id=run_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = []\n",
    "for di in mdict[name]:\n",
    "    dms = di['analysis'].get_report()['result']['design_matrix']\n",
    "    for dm in dms:\n",
    "        df = pd.read_csv(dm)\n",
    "        corrs.append(df['any_faces'].corr(df['speech']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5569070716274516"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5726311513890121"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1891768047871348"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raiders, raiders, PENDING, spcpc\n",
      "SchematicNarrative, perception, PENDING, n7v25\n",
      "SherlockMerlin, MerlinMovie, PASSED, bju9h\n",
      "SherlockMerlin, SherlockMovie, PASSED, 9rup3\n",
      "Sherlock, sherlockPart1, PASSED, hvdjn\n",
      "LearningTemporalStructure, movie, PENDING, fcwn8\n",
      "Budapest, movie, PENDING, 4p9ps\n",
      "NaturalisticNeuroimagingDatabase, 12yearsaslave, PASSED, 83x35\n",
      "NaturalisticNeuroimagingDatabase, 500daysofsummer, PASSED, pr9jk\n",
      "NaturalisticNeuroimagingDatabase, backtothefuture, PASSED, 54hrh\n",
      "NaturalisticNeuroimagingDatabase, citizenfour, PASSED, j4tmp\n",
      "NaturalisticNeuroimagingDatabase, littlemisssunshine, PASSED, r64jf\n",
      "NaturalisticNeuroimagingDatabase, pulpfiction, PASSED, ay2pw\n",
      "NaturalisticNeuroimagingDatabase, split, PASSED, fmkdi\n",
      "NaturalisticNeuroimagingDatabase, theprestige, PASSED, zy9sp\n",
      "NaturalisticNeuroimagingDatabase, theshawshankredemption, PASSED, qsevk\n",
      "NaturalisticNeuroimagingDatabase, theusualsuspects, PASSED, q58ai\n"
     ]
    }
   ],
   "source": [
    "for analysis_dict in mdict[name]:\n",
    "    analysis = analysis_dict['analysis']\n",
    "    if analysis.get_status()['status'] in 'DRAFT':\n",
    "        analysis.compile()\n",
    "    else:\n",
    "        print(f\"{analysis_dict['dataset']}, {analysis_dict['task']}, {analysis.status}, {analysis.hash_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spcpc n7v25 bju9h 9rup3 hvdjn fcwn8 4p9ps'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([a['hash_id'] for a in mdict[name] if a['dataset'] != 'NaturalisticNeuroimagingDatabase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'83x35 pr9jk 54hrh j4tmp r64jf ay2pw fmkdi zy9sp qsevk q58ai'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([a['hash_id'] for a in mdict[name] if a['dataset'] == 'NaturalisticNeuroimagingDatabase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### any_faces + speech + face_time_cum (includes NNDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf_vars = ['any_faces', 'speech', 'log_mean_face_time_cum']\n",
    "name = '+'.join(hrf_vars)\n",
    "\n",
    "# mdict[name] = create_set_models(predictors=hrf_vars, confounds=confounds, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raiders, raiders, PASSED, a9b39\n",
      "SchematicNarrative, perception, PASSED, s4iks\n",
      "SherlockMerlin, MerlinMovie, PASSED, 85tf7\n",
      "SherlockMerlin, SherlockMovie, PASSED, h4bs4\n",
      "Sherlock, sherlockPart1, PASSED, 5gneb\n",
      "LearningTemporalStructure, movie, PASSED, 3i8ro\n",
      "Budapest, movie, PASSED, osho8\n",
      "NaturalisticNeuroimagingDatabase, 12yearsaslave, PASSED, ci2ha\n",
      "NaturalisticNeuroimagingDatabase, 500daysofsummer, PASSED, 978ux\n",
      "NaturalisticNeuroimagingDatabase, backtothefuture, PASSED, 327hp\n",
      "NaturalisticNeuroimagingDatabase, citizenfour, PASSED, 4b8bj\n",
      "NaturalisticNeuroimagingDatabase, littlemisssunshine, PASSED, qpzch\n",
      "NaturalisticNeuroimagingDatabase, pulpfiction, PASSED, 8c62p\n",
      "NaturalisticNeuroimagingDatabase, split, PASSED, 6quaw\n",
      "NaturalisticNeuroimagingDatabase, theprestige, PASSED, 6bvcp\n",
      "NaturalisticNeuroimagingDatabase, theshawshankredemption, PASSED, npcgr\n",
      "NaturalisticNeuroimagingDatabase, theusualsuspects, PASSED, 4y66v\n"
     ]
    }
   ],
   "source": [
    "for analysis_dict in mdict[name]:\n",
    "    analysis = analysis_dict['analysis']\n",
    "    if analysis.get_status()['status'] in 'DRAFT':\n",
    "        analysis.compile()\n",
    "    else:\n",
    "        print(f\"{analysis_dict['dataset']}, {analysis_dict['task']}, {analysis.status}, {analysis.hash_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([an['hash_id'] for an in mdict[name]\n",
    "          if an['analysis'].get_status()['status'] == 'PASSED' and not an['analysis'].get_uploads()\n",
    "         and len(an['analysis'].runs) > 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6quaw 4y66v'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([an['hash_id'] for an in mdict[name]\n",
    "          if an['analysis'].get_status()['status'] == 'PASSED' and not an['analysis'].get_uploads()\n",
    "         and an['dataset'] == 'NaturalisticNeuroimagingDatabase' and len(an['analysis'].runs) < 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([an['hash_id'] for an in mdict[name]\n",
    "          if an['analysis'].get_status()['status'] == 'PASSED' and not an['analysis'].get_uploads()\n",
    "         and an['dataset'] == 'NaturalisticNeuroimagingDatabase' and len(an['analysis'].runs) > 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([an['hash_id'] for an in mdict[name]\n",
    "          if an['analysis'].get_status()['status'] == 'PASSED' and not an['analysis'].get_uploads()\n",
    "         and len(an['analysis'].runs) < 30 and an['dataset'] != 'NaturalisticNeuroimagingDatabase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### any_faces + shot_change + speech + time_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrf_vars = ['any_faces', 'speech', 'shot_change','log_mean_face_time_cum']\n",
    "name = '+'.join(hrf_vars)\n",
    "\n",
    "# mdict[name] = create_set_models(\n",
    "#     predictors=hrf_vars, confounds=confounds, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Budapest, PASSED, h57v2\n",
      "LearningTemporalStructure, PASSED, oyndv\n",
      "Raiders, PASSED, ggic7\n",
      "SchematicNarrative, PASSED, n3pj7\n",
      "Sherlock, PASSED, nyb95\n",
      "SherlockMerlin, PASSED, dci4u\n",
      "SherlockMerlin, PASSED, ovbpo\n"
     ]
    }
   ],
   "source": [
    "for analysis_dict in mdict[name]:\n",
    "    analysis = analysis_dict['analysis']\n",
    "    if analysis.get_status()['status'] in 'DRAFT':\n",
    "        analysis.compile()\n",
    "    else:\n",
    "        print(f\"{analysis_dict['dataset']}, {analysis_dict['task']}, {analysis.status}, {analysis.hash_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([an['hash_id'] for an in mdict[name]\n",
    "          if an['analysis'].get_status()['status'] == 'PASSED' and not an['analysis'].get_uploads()\n",
    "         and len(an['analysis'].runs) > 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([an['hash_id'] for an in mdict[name]\n",
    "          if an['analysis'].get_status()['status'] == 'PASSED' and not an['analysis'].get_uploads()\n",
    "         and len(an['analysis'].runs) < 30])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
